import re
import os
import pyspark.sql.functions as F
import requests

def check_path_exists(path):
    """
    Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n c√≥ t·ªìn t·∫°i hay kh√¥ng
    """
    dbfs_path = path.replace("file://", "") 
    if os.path.exists(dbfs_path):
        print(f"‚úÖ ƒê∆∞·ªùng d·∫´n {path} t·ªìn t·∫°i!")
        return True
    else:
        print(f"‚ùå ƒê∆∞·ªùng d·∫´n {path} kh√¥ng t·ªìn t·∫°i!")
        return False

def check_file_type(path, extension, compression=None):
    """
    Ki·ªÉm tra:
    - Th∆∞ m·ª•c c√≥ ch·ª©a √≠t nh·∫•t 1 file v·ªõi ƒëu√¥i m·ªü r·ªông mong mu·ªën kh√¥ng?
    - N·∫øu l√† parquet v√† c√≥ truy·ªÅn expected_compression => ki·ªÉm tra codec n√©n
    
    Args:
        path (str): ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a file (n√™n l√† file:///dbfs/...)
        extension (str): v√≠ d·ª• ".parquet", ".csv", ".json"
        spark (SparkSession): b·∫Øt bu·ªôc n·∫øu d√πng ki·ªÉm tra codec parquet
        expected_compression (str): "snappy", "gzip", "uncompressed", ...
    """
    dbfs_path = path.replace("file://", "") 
    files = os.listdir(dbfs_path)
    check_partten = extension

    if compression is not None:
        if compression == "gzip":
            check_partten = f"gz.{check_partten}"
        else:
            check_partten = f"{compression}.{check_partten}"

    files = [f for f in files if f.endswith(check_partten)]
    if not files:
        print(f"‚ùå Kh√¥ng c√≥ file ƒë·ªãnh d·∫°ng {check_partten} trong {path}")
        return False
    else:
        print(f"‚úÖ C√≥ file ƒë·ªãnh d·∫°ng {check_partten} trong {path}!")
        return True

def read_data(spark, path, extension, rowTag=None, compression=None):
    reader = spark.read

    if compression is not None:
        reader = reader.option("compression", compression)

    if extension == "parquet":
        return reader.parquet(path)
    elif extension == "csv":
        return reader.option("header", "true").csv(path)
    elif extension == "json":
        return reader.json(path)
    elif extension == "xml":
        if rowTag is None:
            print("‚ùå ƒê·ªãnh d·∫°ng XML y√™u c·∫ßu truy·ªÅn v√†o rowTag.")
            return None
        return reader.format("xml").option("rowTag", rowTag).load(path)
    else:
        print(f"‚ùå Extension '{extension}' kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£.")
        return None

def check_schema_is_correct(path, base_path, spark, extension, rowTag=None, compression=None):
    
    base_data = read_data(path, base_path, spark, extension, rowTag, compression)
    check_data = read_data(path, base_path, spark, extension, rowTag, compression)

    if base_data is None or check_data is None:
        return False

    base_schema = base_data.schema
    check_schema = check_data.schema

    if base_schema == check_schema:
        print(f"‚úÖ Schema c·ªßa file t·∫°i {path} ƒë√∫ng.")
        return True
    else:
        print(f"‚ùå Schema kh√¥ng kh·ªõp!")
        print("Schema g·ªëc:")
        print(base_schema)
        print("Schema ki·ªÉm tra:")
        print(check_schema)
        return False


def is_partitioned_path(path, spark):
    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
    hadoop_path = spark._jvm.org.apache.hadoop.fs.Path(path)
    
    if not fs.exists(hadoop_path):
        print(f"‚ùå ƒê∆∞·ªùng d·∫´n kh√¥ng t·ªìn t·∫°i: {path}")
        return False
    
    # L·∫•y danh s√°ch th∆∞ m·ª•c con
    status = fs.listStatus(hadoop_path)
    dirs = [f.getPath().getName() for f in status if f.isDirectory()]

    # Ki·ªÉm tra xem c√≥ th∆∞ m·ª•c d·∫°ng 'key=value' kh√¥ng
    for d in dirs:
        if re.match(r"[^=]+=[^=]+", d):
            print(f"‚úÖ ƒê∆∞·ªùng d·∫´n ch·ª©a partitionBy: {d}")
            return True

    print("‚ùå ƒê∆∞·ªùng d·∫´n KH√îNG ch·ª©a partitionBy.")
    return False



def check_sorted_descending(path, column_name, spark):
    df = spark.read.parquet(path)
    values = [row[column_name] for row in df.select(column_name).collect()]
    if all(values[i] >= values[i+1] for i in range(len(values)-1)):
        print(f"‚úÖ D·ªØ li·ªáu ƒëang ƒë∆∞·ª£c s·∫Øp x·∫øp gi·∫£m d·∫ßn!")
        return True
    else:
        print(f"‚ùå D·ªØ li·ªáu ƒëang KH√îNG s·∫Øp x·∫øp gi·∫£m d·∫ßn!")
        return False

import re

def check_content_files(path, base_path, spark, extension, rowTag=None, compression=None):
    try:
        # ƒê·ªçc d·ªØ li·ªáu
        base_data = read_data(path, base_path, spark, extension, rowTag, compression)
        check_data = read_data(path, base_path, spark, extension, rowTag, compression)

        # Ki·ªÉm tra schema
        if base_data.schema != check_data.schema:
            print("‚ùå Schema kh√¥ng kh·ªõp, kh√¥ng th·ªÉ so s√°nh n·ªôi dung!")
            print("üîß Schema file g·ªëc:")
            base_data.printSchema()
            print("üîç Schema file ki·ªÉm tra:")
            check_data.printSchema()
            return

        # So s√°nh n·ªôi dung
        missing_from_check = base_data.exceptAll(check_data)
        extra_in_check = check_data.exceptAll(base_data)

        missing_count = missing_from_check.count()
        extra_count = extra_in_check.count()

        if missing_count == 0 and extra_count == 0:
            print("‚úÖ D·ªØ li·ªáu ch√≠nh x√°c!")
        else:
            print("‚ùå D·ªØ li·ªáu kh√¥ng ch√≠nh x√°c!")
            if missing_count > 0:
                print(f"‚ö†Ô∏è Thi·∫øu {missing_count} d√≤ng so v·ªõi d·ªØ li·ªáu g·ªëc.")
            if extra_count > 0:
                print(f"‚ö†Ô∏è Th·ª´a {extra_count} d√≤ng so v·ªõi d·ªØ li·ªáu g·ªëc.")

    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω: {e}")

def list_files_in_github_folder(repo_owner, repo_name, folder_path, branch="main"):
    api_url = f"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{folder_path}?ref={branch}"
    response = requests.get(api_url)
    
    if response.status_code == 200:
        contents = response.json()
        return [item['path'] for item in contents if item['type'] == 'file']
    else:
        print(f"Failed to list files in {folder_path}, Status Code: {response.status_code}")
        return []

def download_github_file(repo_owner, repo_name, file_path, save_path, branch="main"):
    raw_url = f"https://raw.githubusercontent.com/{repo_owner}/{repo_name}/{branch}/{file_path}"
    response = requests.get(raw_url)

    if response.status_code == 200:
        dbfs_path = f"/dbfs{save_path}"
        os.makedirs(os.path.dirname(dbfs_path), exist_ok=True)
        with open(dbfs_path, "wb") as file:
            file.write(response.content)
        print(f"Downloaded: {file_path} to {save_path}")
    else:
        print(f"Failed to download {file_path}, Status Code: {response.status_code}")

