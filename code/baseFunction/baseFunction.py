import re
import os

def check_path_exists(path):
    """
    Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n c√≥ t·ªìn t·∫°i hay kh√¥ng
    """
    dbfs_path = path.replace("file://", "") 
    if os.path.exists(dbfs_path):
        print(f"‚úÖ ƒê∆∞·ªùng d·∫´n {path} t·ªìn t·∫°i!")
        return True
    else:
        print(f"‚ùå ƒê∆∞·ªùng d·∫´n {path} kh√¥ng t·ªìn t·∫°i!")
        return False

def check_file_type(path, extension):
    """
    Ki·ªÉm tra t·∫•t c·∫£ file parquet trong path c√≥ ƒëang s·ª≠ d·ª•ng Snappy compression hay kh√¥ng
    """
    dbfs_path = path.replace("file://", "") 
    files = os.listdir(dbfs_path)
    parquet_files = [f for f in files if f.endswith(extension)]
    if not parquet_files:
        print(f"‚ùå Kh√¥ng c√≥ file ƒë·ªãnh d·∫°ng {extension} trong {path}")
        return False
    else:
        print(f"‚úÖ C√≥ file ƒë·ªãnh d·∫°ng {extension} trong {path}!")
        return True

def check_schema_is_correct(path, base_path, spark, extension, rowTag=None):
    if extension == "parquet":
        base_data = spark.read.parquet(base_path)
        check_data = spark.read.parquet(path)
    elif extension == "csv":
        base_data = spark.read.option("header", "true").csv(base_path)
        check_data = spark.read.option("header", "true").csv(path)
    elif extension == "json":
        base_data = spark.read.json(base_path)
        check_data = spark.read.json(path)
    elif extension == "xml":
        if rowTag is None:
            print("‚ùå ƒê·ªãnh d·∫°ng XML y√™u c·∫ßu truy·ªÅn v√†o rowTag.")
            return False
        base_data = spark.read.format("xml").option("rowTag", rowTag).load(base_path)
        check_data = spark.read.format("xml").option("rowTag", rowTag).load(path)
    else:
        print(f"‚ùå Extension '{extension}' kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£.")
        return False

    base_schema = base_data.schema
    check_schema = check_data.schema

    if base_schema == check_schema:
        print(f"‚úÖ Schema c·ªßa file t·∫°i {path} ƒë√∫ng.")
        return True
    else:
        print(f"‚ùå Schema kh√¥ng kh·ªõp!")
        print("Schema g·ªëc:")
        base_schema.printTreeString()
        print("Schema ki·ªÉm tra:")
        check_schema.printTreeString()
        return False


def is_partitioned_path(path, spark):
    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
    hadoop_path = spark._jvm.org.apache.hadoop.fs.Path(path)
    
    if not fs.exists(hadoop_path):
        print(f"‚ùå ƒê∆∞·ªùng d·∫´n kh√¥ng t·ªìn t·∫°i: {path}")
        return False
    
    # L·∫•y danh s√°ch th∆∞ m·ª•c con
    status = fs.listStatus(hadoop_path)
    dirs = [f.getPath().getName() for f in status if f.isDirectory()]

    # Ki·ªÉm tra xem c√≥ th∆∞ m·ª•c d·∫°ng 'key=value' kh√¥ng
    for d in dirs:
        if re.match(r"[^=]+=[^=]+", d):
            print(f"‚úÖ ƒê∆∞·ªùng d·∫´n ch·ª©a partitionBy: {d}")
            return True

    print("‚ùå ƒê∆∞·ªùng d·∫´n KH√îNG ch·ª©a partitionBy.")
    return False



def check_sorted_descending(path, column_name, spark):
    df = spark.read.parquet(path)
    values = [row[column_name] for row in df.select(column_name).collect()]
    if all(values[i] >= values[i+1] for i in range(len(values)-1)):
        print(f"‚úÖ D·ªØ li·ªáu ƒëang ƒë∆∞·ª£c s·∫Øp x·∫øp gi·∫£m d·∫ßn!")
        return True
    else:
        print(f"‚ùå D·ªØ li·ªáu ƒëang KH√îNG s·∫Øp x·∫øp gi·∫£m d·∫ßn!")
        return False

import re

def check_content_files(path, base_path, spark, extension, rowTag=None):
    def read_data(p):
        if extension == "parquet":
            return spark.read.parquet(p)
        elif extension == "csv":
            return spark.read.option("header", "true").csv(p)
        elif extension == "json":
            return spark.read.json(p)
        elif extension == "xml":
            if rowTag is None:
                raise ValueError("V·ªõi ƒë·ªãnh d·∫°ng XML, c·∫ßn truy·ªÅn v√†o rowTag.")
            return spark.read.format("xml").option("rowTag", rowTag).load(p)
        else:
            raise ValueError(f"‚ùå ƒê·ªãnh d·∫°ng '{extension}' kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£.")

    try:
        # ƒê·ªçc d·ªØ li·ªáu
        base_data = read_data(base_path)
        check_data = read_data(path)

        # Ki·ªÉm tra schema
        if base_data.schema != check_data.schema:
            print("‚ùå Schema kh√¥ng kh·ªõp, kh√¥ng th·ªÉ so s√°nh n·ªôi dung!")
            print("üîß Schema file g·ªëc:")
            base_data.printSchema()
            print("üîç Schema file ki·ªÉm tra:")
            check_data.printSchema()
            return

        # So s√°nh n·ªôi dung
        missing_from_check = base_data.exceptAll(check_data)
        extra_in_check = check_data.exceptAll(base_data)

        missing_count = missing_from_check.count()
        extra_count = extra_in_check.count()

        if missing_count == 0 and extra_count == 0:
            print("‚úÖ D·ªØ li·ªáu ch√≠nh x√°c!")
        else:
            print("‚ùå D·ªØ li·ªáu kh√¥ng ch√≠nh x√°c!")
            if missing_count > 0:
                print(f"‚ö†Ô∏è Thi·∫øu {missing_count} d√≤ng so v·ªõi d·ªØ li·ªáu g·ªëc.")
            if extra_count > 0:
                print(f"‚ö†Ô∏è Th·ª´a {extra_count} d√≤ng so v·ªõi d·ªØ li·ªáu g·ªëc.")

    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω: {e}")
